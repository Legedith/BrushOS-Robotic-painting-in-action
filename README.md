# Gemini 3 in the Physical World

We gave Gemini 3 **eyes and hands** and asked a hard question: **can it understand a real scene and act autonomously?**  
This project is the answer. Gemini 3 sees the workspace, plans strokes, moves a real robot arm, checks the result, and improves.  
It is not a simulation. It is not pixels. It is **ink on paper**.

We then pushed it into art. The result is, to our knowledge, the **first Gemini 3-powered real-world drawing pipeline**: composition generated by Gemini 3, executed in the physical world, and refined with live vision feedback.

## Evidence (Images + Logs)

![Drawing tools visible](docs/evidence/drawing_tools_check.jpg)
Tools check: Gemini 3 verified the brush in the gripper, the paint bowl, and the paper.

![Cave art v1](docs/evidence/final_cave_art_v1.jpg)
Cave art v1: the agent drew a bison and two hunters from strokes it generated.

![Cave art v2](docs/evidence/final_cave_art_v2.jpg)
Cave art v2: the agent added a sun and ground line to improve composition.

![Cave art v3](docs/evidence/final_cave_art_v3.jpg)
Cave art v3: final pass with birds + mountains for scene depth.

Run transcript: `docs/logs/run.logs`

```text
Tool call: build_bezier_stroke {"p0_u": 0.35, "p0_v": 0.45, ...}
Tool call: concat_strokes {"strokes_json": "[[[0.7,0.45],[0.75,0.55],[0.7,0.6]], ...]"}
Tool call: arm_draw_strokes {"strokes_json": "[[[0.35,0.45],...]]", "refill_after": 2}
Tool call: capture_photo_tool {"prefix": "final_cave_art_result", "output_dir": "captures\\agent_runs\\20260209_183722_draw_square_in_air"}
Tool output: arm_draw_strokes {"status": "success", "strokes": 8, "poses_file": "data/poses.json"}
```

## What This Proves

- Gemini 3 can map a real scene into actionable plans, not just describe it.
- Gemini 3 can generate geometry, execute it physically, and correct based on feedback.
- Multimodal reasoning is not a demo slide. It is **action**.

## Gemini Integration (200-Word Summary)

Gemini 3 is the decision engine for a closed-loop physical drawing system. The model receives live camera frames and produces structured tool calls rather than freeform text. It identifies the brush, bowl, and paper, reasons about spatial layout, and decides when to move, refill paint, or draw strokes. Gemini 3's multimodal reasoning is used in three phases: (1) perception to locate objects and the drawing area, (2) planning to generate large-stroke geometry and sequence it into actions, and (3) evaluation to compare new images to earlier ones and decide whether to add or correct strokes.

The drawing pipeline is intentionally constrained to big, continuous strokes to match a real brush. Gemini 3 generates strokes through geometry primitives (circles, arcs, S-curves, Beziers), transforms them, concatenates them, and executes them through the robot arm. After execution, Gemini 3 captures new images and refines the composition. This connects perception, reasoning, and motor control in the physical world and demonstrates Gemini 3's multimodal autonomy beyond any chat interface.

## Technical Journey (Short)

1. Pose teaching  
Built a learning-mode tool to capture paper corners and bowl poses into `data/poses.json`.

2. Drawing primitives  
Implemented normalized stroke execution and refill logic for a thick brush.

3. Autonomous agent  
Gave Gemini 3 building blocks to invent shapes and execute them without presets.

4. Closed-loop refinement  
The agent captured images after major strokes and improved the scene iteratively.

## Challenges and Fixes

- Thick brush made thin details disappear. We enforced large strokes and frequent refills.
- Pose accuracy dominated quality. We standardized a four-corner paper frame.
- Visual feedback was essential. The agent captures and evaluates after major actions.
- The robot server warned about version mismatch. We pinned `pyniryo` 1.2.3 and documented upgrades.

## Architecture (High Level)

Camera -> Gemini 3 reasoning -> tool calls -> robot execution -> camera feedback -> refinement loop

## Repo Layout

- `camera_agent/`: Gemini 3 agent + drawing tools
- `scripts/`: runnable entrypoints
- `data/poses.json`: taught poses
- `docs/evidence/`: curated images
- `docs/logs/run.logs`: run transcript

## How to Run

```bash
uv sync
uv run scripts/teach_poses.py --ip 10.10.10.10 --poses-file data/poses.json
uv run scripts/run_camera_agent_interactive.py
```

Example prompt:

```
Design a large cave-art scene with big strokes only.
Refill after every 1-2 strokes. Capture after each major addition.
```

## Hackathon Submission Checklist

- Gemini Integration summary: see "Gemini Integration" section above.
- Public project link: TODO
- Public code repository: this repo
- 3-minute demo video: TODO

## How We Meet Judging Criteria

Technical Execution: closed-loop system, real robot control, vision feedback, robust tools.  
Potential Impact: demonstrates real-world autonomy with multimodal reasoning and physical actuation.  
Innovation / Wow: Gemini 3 drives physical art and self-corrects from live vision.  
Presentation / Demo: evidence images, run logs, and clear instructions included.

## Environment Variables

- `GOOGLE_API_KEY` or `GOOGLE_API_KEYS`: required for Gemini
- `NIRYO_ROBOT_IP`: optional override for robot IP (default `10.10.10.10`)
